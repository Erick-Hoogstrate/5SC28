{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cace89df",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import gym\n",
    "import pickle\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "env = gym.make(\"Pendulum-v1\")\n",
    "\n",
    "ROOT_DIR = os.path.dirname('')\n",
    "LOAD_PRETRAINED = True\n",
    "VALIDATING = True\n",
    "LEARNING_RATE = 0.1\n",
    "DISCOUNT = 0.95\n",
    "EPISODES = 150000\n",
    "SHOW_EVERY = 100\n",
    "STATS_EVERY = 100\n",
    "epsilon = 1\n",
    "EPSILON_THRESHOLD = 0.1\n",
    "epsilon_decay_value = 0.999"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d6097fc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Making discrete action space\n",
    "DISCRETE_ACTION_SPACE_SIZE = 26\n",
    "discrete_action_space_win_size = (env.action_space.high - env.action_space.low) / (DISCRETE_ACTION_SPACE_SIZE - 1)\n",
    "action_space = {}\n",
    "for i in range(DISCRETE_ACTION_SPACE_SIZE):\n",
    "    action_space[i] = [env.action_space.low[0] + (i * discrete_action_space_win_size[0])]\n",
    "\n",
    "# # Making discrete observation space\n",
    "DISCRETE_OS_SIZE = [21, 21, 65]\n",
    "discrete_os_win_size = (env.observation_space.high - env.observation_space.low) / [i-1 for i in DISCRETE_OS_SIZE]\n",
    "if LOAD_PRETRAINED:\n",
    "    q_table = np.load(os.path.realpath(os.path.join(ROOT_DIR, 'qtable.npy')))\n",
    "else:\n",
    "    q_table = np.random.uniform(low=-2, high=-0, size=(DISCRETE_OS_SIZE + [DISCRETE_ACTION_SPACE_SIZE]))\n",
    "ep_rewards = []\n",
    "aggr_ep_rewards = {'ep': [], 'avg': [], 'max': [], 'min': []}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "90639b35",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_discrete_state(state):\n",
    "    ds = (state - env.observation_space.low) / discrete_os_win_size\n",
    "    return tuple(ds.astype(np.int32))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6a51dce3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "showing episode 0\n",
      "Episode:     0, average reward: -14.0, current epsilon: 1.00\n",
      "showing episode 100\n",
      "Episode:   100, average reward: -1288.7, current epsilon: 0.90\n",
      "showing episode 200\n",
      "Episode:   200, average reward: -1232.4, current epsilon: 0.82\n",
      "showing episode 300\n",
      "Episode:   300, average reward: -1186.7, current epsilon: 0.74\n",
      "showing episode 400\n",
      "Episode:   400, average reward: -1182.3, current epsilon: 0.67\n",
      "showing episode 500\n",
      "Episode:   500, average reward: -1204.7, current epsilon: 0.61\n",
      "showing episode 600\n",
      "Episode:   600, average reward: -1258.1, current epsilon: 0.55\n",
      "showing episode 700\n",
      "Episode:   700, average reward: -1222.3, current epsilon: 0.50\n",
      "showing episode 800\n",
      "Episode:   800, average reward: -1205.7, current epsilon: 0.45\n",
      "showing episode 900\n",
      "Episode:   900, average reward: -1179.0, current epsilon: 0.41\n",
      "showing episode 1000\n",
      "Episode:  1000, average reward: -1200.1, current epsilon: 0.37\n",
      "showing episode 1100\n",
      "Episode:  1100, average reward: -1259.8, current epsilon: 0.33\n",
      "showing episode 1200\n",
      "Episode:  1200, average reward: -1236.4, current epsilon: 0.30\n",
      "showing episode 1300\n",
      "Episode:  1300, average reward: -1195.6, current epsilon: 0.27\n",
      "showing episode 1400\n",
      "Episode:  1400, average reward: -1197.9, current epsilon: 0.25\n",
      "showing episode 1500\n",
      "Episode:  1500, average reward: -1180.7, current epsilon: 0.22\n",
      "showing episode 1600\n",
      "Episode:  1600, average reward: -1212.6, current epsilon: 0.20\n",
      "showing episode 1700\n",
      "Episode:  1700, average reward: -1182.2, current epsilon: 0.18\n",
      "showing episode 1800\n",
      "Episode:  1800, average reward: -1176.5, current epsilon: 0.16\n",
      "showing episode 1900\n",
      "Episode:  1900, average reward: -1086.0, current epsilon: 0.15\n",
      "showing episode 2000\n",
      "Episode:  2000, average reward: -1116.8, current epsilon: 0.14\n",
      "showing episode 2100\n",
      "Episode:  2100, average reward: -1090.4, current epsilon: 0.12\n",
      "showing episode 2200\n",
      "Episode:  2200, average reward: -1148.2, current epsilon: 0.11\n",
      "showing episode 2300\n",
      "Episode:  2300, average reward: -1142.5, current epsilon: 0.10\n",
      "showing episode 2400\n",
      "Episode:  2400, average reward: -1208.1, current epsilon: 0.10\n",
      "showing episode 2500\n",
      "Episode:  2500, average reward: -1145.0, current epsilon: 0.10\n",
      "showing episode 2600\n",
      "Episode:  2600, average reward: -1192.8, current epsilon: 0.10\n",
      "showing episode 2700\n",
      "Episode:  2700, average reward: -1147.3, current epsilon: 0.10\n",
      "showing episode 2800\n",
      "Episode:  2800, average reward: -1133.8, current epsilon: 0.10\n",
      "showing episode 2900\n",
      "Episode:  2900, average reward: -1154.9, current epsilon: 0.10\n",
      "showing episode 3000\n",
      "Episode:  3000, average reward: -1138.7, current epsilon: 0.10\n",
      "showing episode 3100\n",
      "Episode:  3100, average reward: -1159.4, current epsilon: 0.10\n",
      "showing episode 3200\n",
      "Episode:  3200, average reward: -1061.3, current epsilon: 0.10\n",
      "showing episode 3300\n",
      "Episode:  3300, average reward: -1184.8, current epsilon: 0.10\n",
      "showing episode 3400\n",
      "Episode:  3400, average reward: -1138.0, current epsilon: 0.10\n",
      "showing episode 3500\n",
      "Episode:  3500, average reward: -1098.1, current epsilon: 0.10\n",
      "showing episode 3600\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[5], line 26\u001b[0m\n\u001b[0;32m     24\u001b[0m new_discrete_state \u001b[38;5;241m=\u001b[39m get_discrete_state(new_state)\n\u001b[0;32m     25\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m episode \u001b[38;5;241m%\u001b[39m SHOW_EVERY \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m---> 26\u001b[0m     \u001b[43menv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrender\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     27\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m VALIDATING:\n\u001b[0;32m     28\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m done:\n",
      "File \u001b[1;32m~\\AppData\\Local\\anaconda3\\envs\\ml4sc\\lib\\site-packages\\gym\\core.py:295\u001b[0m, in \u001b[0;36mWrapper.render\u001b[1;34m(self, mode, **kwargs)\u001b[0m\n\u001b[0;32m    294\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mrender\u001b[39m(\u001b[38;5;28mself\u001b[39m, mode\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhuman\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m--> 295\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39menv\u001b[38;5;241m.\u001b[39mrender(mode, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32m~\\AppData\\Local\\anaconda3\\envs\\ml4sc\\lib\\site-packages\\gym\\envs\\classic_control\\pendulum.py:85\u001b[0m, in \u001b[0;36mPendulumEnv.render\u001b[1;34m(self, mode)\u001b[0m\n\u001b[0;32m     82\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlast_u \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m     83\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mimgtrans\u001b[38;5;241m.\u001b[39mscale \u001b[38;5;241m=\u001b[39m (\u001b[38;5;241m-\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlast_u \u001b[38;5;241m/\u001b[39m \u001b[38;5;241m2\u001b[39m, np\u001b[38;5;241m.\u001b[39mabs(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlast_u) \u001b[38;5;241m/\u001b[39m \u001b[38;5;241m2\u001b[39m)\n\u001b[1;32m---> 85\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mviewer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrender\u001b[49m\u001b[43m(\u001b[49m\u001b[43mreturn_rgb_array\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmode\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m==\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mrgb_array\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Local\\anaconda3\\envs\\ml4sc\\lib\\site-packages\\gym\\envs\\classic_control\\rendering.py:145\u001b[0m, in \u001b[0;36mViewer.render\u001b[1;34m(self, return_rgb_array)\u001b[0m\n\u001b[0;32m    143\u001b[0m     arr \u001b[38;5;241m=\u001b[39m arr\u001b[38;5;241m.\u001b[39mreshape(buffer\u001b[38;5;241m.\u001b[39mheight, buffer\u001b[38;5;241m.\u001b[39mwidth, \u001b[38;5;241m4\u001b[39m)\n\u001b[0;32m    144\u001b[0m     arr \u001b[38;5;241m=\u001b[39m arr[::\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, :, \u001b[38;5;241m0\u001b[39m:\u001b[38;5;241m3\u001b[39m]\n\u001b[1;32m--> 145\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwindow\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mflip\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    146\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39monetime_geoms \u001b[38;5;241m=\u001b[39m []\n\u001b[0;32m    147\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m arr \u001b[38;5;28;01mif\u001b[39;00m return_rgb_array \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39misopen\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python310\\site-packages\\pyglet\\window\\win32\\__init__.py:379\u001b[0m, in \u001b[0;36mWin32Window.flip\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    377\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_always_dwm \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dwm_composition_enabled():\n\u001b[0;32m    378\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_interval:\n\u001b[1;32m--> 379\u001b[0m             \u001b[43m_dwmapi\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mDwmFlush\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    381\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstyle \u001b[38;5;129;01min\u001b[39;00m (\u001b[38;5;124m'\u001b[39m\u001b[38;5;124moverlay\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtransparent\u001b[39m\u001b[38;5;124m'\u001b[39m):\n\u001b[0;32m    382\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mupdate_transparency()\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "if VALIDATING:\n",
    "    epsilon = 0\n",
    "\n",
    "for episode in range(EPISODES):\n",
    "    episode_reward = 0\n",
    "    discrete_state = get_discrete_state(env.reset())\n",
    "    done = False\n",
    "\n",
    "    if episode % SHOW_EVERY == 0:\n",
    "        render = True\n",
    "        print(f'showing episode {episode}')\n",
    "    else:\n",
    "        render = False\n",
    "    while not done:\n",
    "        if np.random.random() > epsilon:\n",
    "            # Get action from Q table\n",
    "            action = np.argmax(q_table[discrete_state])\n",
    "        else:\n",
    "            # Get random action\n",
    "            action = np.random.randint(0, DISCRETE_ACTION_SPACE_SIZE)\n",
    "        torque = action_space[action]\n",
    "        new_state, reward, done, _ = env.step(torque)\n",
    "        episode_reward += reward\n",
    "        new_discrete_state = get_discrete_state(new_state)\n",
    "        if episode % SHOW_EVERY == 0:\n",
    "            env.render()\n",
    "        if not VALIDATING:\n",
    "            if not done:\n",
    "                max_future_q = np.max(q_table[new_discrete_state])\n",
    "                current_q = q_table[discrete_state + (action,)]\n",
    "                # And here's our equation for a new Q value for current state and action\n",
    "                new_q = (1 - LEARNING_RATE) * current_q + LEARNING_RATE * (reward + DISCOUNT * max_future_q)\n",
    "\n",
    "                # Update Q table with new Q value\n",
    "                q_table[discrete_state + (action,)] = new_q\n",
    "\n",
    "            if new_state[0] == 0 and new_state[1] == 1:\n",
    "                q_table[discrete_state + (action,)] = 0\n",
    "                print(f\"Acheived in {episode}\")\n",
    "\n",
    "        discrete_state = new_discrete_state\n",
    "    if not VALIDATING:\n",
    "        # Decaying is being done every episode if episode number is within decaying range\n",
    "        if epsilon >= EPSILON_THRESHOLD:\n",
    "            epsilon *= epsilon_decay_value\n",
    "        ep_rewards.append(episode_reward)\n",
    "        if not episode % STATS_EVERY:\n",
    "            average_reward = sum(ep_rewards[-STATS_EVERY:]) / STATS_EVERY\n",
    "            aggr_ep_rewards['ep'].append(episode)\n",
    "            aggr_ep_rewards['avg'].append(average_reward)\n",
    "            aggr_ep_rewards['max'].append(max(ep_rewards[-STATS_EVERY:]))\n",
    "            aggr_ep_rewards['min'].append(min(ep_rewards[-STATS_EVERY:]))\n",
    "            print(f'Episode: {episode:>5d}, average reward: {average_reward:>4.1f}, current epsilon: {epsilon:>1.2f}')\n",
    "        if episode % 10 == 0:\n",
    "            np.save(os.path.realpath(os.path.join(ROOT_DIR, 'qtable.npy')), q_table)\n",
    "\n",
    "env.close()\n",
    "\n",
    "if not VALIDATING:\n",
    "    filehandler = open(\"statistics\", 'wb')\n",
    "    pickle.dump(aggr_ep_rewards, filehandler)\n",
    "\n",
    "    plt.plot(aggr_ep_rewards['ep'], aggr_ep_rewards['avg'], label=\"average rewards\")\n",
    "    plt.plot(aggr_ep_rewards['ep'], aggr_ep_rewards['max'], label=\"max rewards\")\n",
    "    plt.plot(aggr_ep_rewards['ep'], aggr_ep_rewards['min'], label=\"min rewards\")\n",
    "    plt.legend(loc=4)\n",
    "    plt.savefig(os.path.realpath(os.path.join(ROOT_DIR, \"Statistics.png\")))\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "042ebcda",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
