{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9b34da09",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('gym-unbalanced-disk/gym_unbalanced_disk/')\n",
    "import gym_unbalanced_disk, time, gym.wrappers\n",
    "from matplotlib import pyplot as plt\n",
    "import os\n",
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "from DQNfunctions import Qfunction, DQN_rollout, show, eval_Q, save_Qfunction, load_Qfunction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "95240858",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Indicate whether to train Qfunction or to load existing Qfunction for analysis\n",
    "Train_Qfunction = False\n",
    "\n",
    "# Decide file name for either saving or loading and whether to save Q function and plots\n",
    "fileNameLoad = 'QfunctionFinal.pt'\n",
    "fileNameSave = 'QfunctionFinal.pt'\n",
    "Save = False\n",
    "\n",
    "# Choose reward function either for training or evaluation\n",
    "target_angle = np.pi # target set to be balanced on top\n",
    "\n",
    "angle_reward = lambda self: ((np.cos(self.th-target_angle)+1.5)**2 - 0.25)\n",
    "correctVelocity_reward = lambda self: 0.00125*(((np.cos(self.th)+1)/2)*(self.omega)**2)\n",
    "voltage_penalty = lambda self: -0.01*(((np.sin(self.th - np.pi/2)+1)/2)*(self.u)**2)\n",
    "\n",
    "reward_function = lambda self: angle_reward(self) + correctVelocity_reward(self) + voltage_penalty(self)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63af2e56",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This will only run for training, change variables as you please for training\n",
    "if Train_Qfunction:\n",
    "    max_episode_steps = 250\n",
    "    env = gym.make('unbalanced-disk-sincos-v0', dt=0.025, umax=3.)\n",
    "    env = gym.wrappers.time_limit.TimeLimit(env,max_episode_steps=max_episode_steps)\n",
    "\n",
    "    env.change_reward_function(reward_function)\n",
    "\n",
    "    env.set_discrete_values_manual([-3.0, -1.5, -0.9, -0.35, 0, 0.35, 0.9, 1.5, 3.0])\n",
    "\n",
    "    # These settings can be changed for training\n",
    "    gamma = 0.98 #f=)\n",
    "    batch_size = 256 #f=)\n",
    "    N_iterations = 26 #f=)\n",
    "    N_rollout = 50000 #f=)\n",
    "    N_epochs = 20 #f=)\n",
    "    N_evals = 5 #f=)\n",
    "    lr = 0.0005 #given\n",
    "\n",
    "    assert isinstance(env.action_space,gym.spaces.Discrete), 'action space requires to be discrete'\n",
    "    Q = Qfunction(env)\n",
    "    optimizer = torch.optim.Adam(Q.parameters(),lr=lr) #low learning rate\n",
    "    DQN_rollout(Q, optimizer, env, use_target_net=True, gamma=gamma, N_iterations=N_iterations, \\\n",
    "                N_rollout=N_rollout, N_epochs=N_epochs, N_evals=N_evals)\n",
    "    save_Qfunction(Q, fileNameSave)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf37c6fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "#This will only run if Q function is loaded in.\n",
    "# WARNING: The discrete values should be the same as in the original run to be able to run the loaded Qfunction.\n",
    "# WARNING: The reward function should be the same as in the original run to be able to asses the loaded Qfunction.\n",
    "if not Train_Qfunction:\n",
    "    max_episode_steps = 250\n",
    "    env = gym.make('unbalanced-disk-sincos-v0', dt=0.025, umax=3.)\n",
    "    env = gym.wrappers.time_limit.TimeLimit(env,max_episode_steps=max_episode_steps)\n",
    "    \n",
    "    env.set_discrete_values_manual([-3.0, -1.2, -0.48, -0.19, 0, 0.19, 0.48, 1.2, 3.0])\n",
    "    \n",
    "    env.change_reward_function(reward_function)\n",
    "    \n",
    "    Q = load_Qfunction(env, fileNameLoad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a4b2cba",
   "metadata": {},
   "outputs": [],
   "source": [
    "u,theta,omega,rewards = show(Q, env)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddd063ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a figure with three subplots\n",
    "fig, axs = plt.subplots(2, 2, figsize=(10, 10))\n",
    "\n",
    "# Plot theta\n",
    "axs[0,0].plot([i*env.dt for i in range(max_episode_steps)], theta, '.')\n",
    "axs[0,0].set_title('Theta')\n",
    "axs[0,0].set_xlabel('Time (s)')\n",
    "axs[0,0].set_ylabel('Angle (Rad)')\n",
    "axs[0,0].axhline(y=target_angle, color='r', linestyle='-')\n",
    "axs[0,0].axhline(y=-target_angle, color='r', linestyle='-')\n",
    "axs[0,0].grid(True)\n",
    "\n",
    "# Plot u\n",
    "axs[0,1].plot([i*env.dt for i in range(max_episode_steps)], u, '.')\n",
    "axs[0,1].set_title('u')\n",
    "axs[0,1].set_xlabel('Time (s)')\n",
    "axs[0,1].set_ylabel('Input voltage (V)')\n",
    "axs[0,1].grid(True)\n",
    "\n",
    "# Plot omega\n",
    "axs[1,0].plot([i*env.dt for i in range(max_episode_steps)], omega, '.')\n",
    "axs[1,0].set_title('Omega')\n",
    "axs[1,0].set_xlabel('Time (s)')\n",
    "axs[1,0].set_ylabel('Angular velocity (rad/s)')\n",
    "axs[1,0].grid(True)\n",
    "\n",
    "# Plot omega\n",
    "axs[1,1].plot([i*env.dt for i in range(max_episode_steps)], rewards, '.')\n",
    "axs[1,1].set_title('Rewards')\n",
    "axs[1,1].set_xlabel('Time (s)')\n",
    "axs[1,1].set_ylabel('Reward (*)')\n",
    "axs[1,1].grid(True)\n",
    "\n",
    "# Adjust spacing between subplots\n",
    "plt.tight_layout()\n",
    "\n",
    "# Show theÂ plot\n",
    "plt.show()\n",
    "\n",
    "if Save:\n",
    "    ROOT_DIR = os.path.dirname('')\n",
    "    fig.savefig(os.path.realpath(os.path.join(ROOT_DIR, os.path.splitext(fileNameSave)[0]+'FigureSimulation.eps')), format='eps', bbox_inches='tight')\n",
    "    fig.savefig(os.path.realpath(os.path.join(ROOT_DIR, os.path.splitext(fileNameSave)[0]+'FigureSimulation.svg')),bbox_inches='tight')\n",
    "    if Train_Qfunction:\n",
    "        save_Qfunction(Q, fileNameSave)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc7a5889",
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "Rewards = [eval_Q(Q,env) for i in range(250)]\n",
    "plt.plot(Rewards,'.')\n",
    "plt.title(f'Mean={np.mean(Rewards):.4f}, Standard deviation={np.std(Rewards):.4f}')\n",
    "plt.xlabel('instance')\n",
    "plt.ylabel('Reward per episode')\n",
    "plt.ylim(np.min(Rewards)-2.5, np.max(Rewards)+2.5)\n",
    "plt.axhline(y=np.mean(Rewards), color='r', linestyle='-')\n",
    "plt.grid(True)\n",
    "# plt.show()\n",
    "\n",
    "if Save:\n",
    "    ROOT_DIR = os.path.dirname('')\n",
    "    plt.savefig(os.path.realpath(os.path.join(ROOT_DIR, os.path.splitext(fileNameSave)[0]+'Evaluation.eps')), format='eps', bbox_inches='tight')\n",
    "    plt.savefig(os.path.realpath(os.path.join(ROOT_DIR, os.path.splitext(fileNameSave)[0]+'Evaluation.svg')), bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31707176",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
