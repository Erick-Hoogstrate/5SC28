{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "88632180",
   "metadata": {},
   "source": [
    "# Q learning\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "48775bf9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "printing succesful\n"
     ]
    }
   ],
   "source": [
    "from torch import nn\n",
    "import torch, gym, gym_unbalanced_disk, time, gym.wrappers\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "from copy import deepcopy\n",
    "from math import cos\n",
    "\n",
    "print(\"printing succesful\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "afc32c7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Qfunction(nn.Module):\n",
    "    def __init__(self, env):\n",
    "        super(Qfunction,self).__init__()\n",
    "        self.lay1 = nn.Linear(env.observation_space.shape[0], 40) #a)\n",
    "        self.F1 =  nn.Tanh() #a)\n",
    "        self.lay2 = nn.Linear(40,1) # 1 for continuous, env.action_space.n for discrete\n",
    "    \n",
    "    def forward(self, obs):\n",
    "        return self.lay2(self.F1(self.lay1(obs))) #a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "15e7abad",
   "metadata": {},
   "outputs": [],
   "source": [
    "def show(Q,env):\n",
    "    with torch.no_grad():\n",
    "        #you can use Qfun(obs) as a shorthand for the q function.\n",
    "        Qfun = lambda x: Q(torch.tensor(x[None,:],dtype=torch.float32))[0].numpy() #convert x to torch.tensor -> put in the Q function -> back to numpy\n",
    "        try:\n",
    "            obs = env.reset() #b)\n",
    "            env.render() #b)\n",
    "            time.sleep(1) #b)\n",
    "            while True: #b)\n",
    "                action = np.argmax(Qfun(obs)) #b)\n",
    "                obs, reward, done, info = env.step(action) #b)\n",
    "                time.sleep(1/60) #b)\n",
    "                env.render() #b)\n",
    "                if done: #b)\n",
    "                    time.sleep(0.5)  #b)\n",
    "                    break  #b)\n",
    "        finally: #this will always run even when an error occurs\n",
    "            env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0eaef30b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def rollout(Q, env, epsilon=0.1, N_rollout=10_000): \n",
    "    #save the following (use .append)\n",
    "    Start_state = [] #hold an array of (x_t)\n",
    "    Actions = [] #hold an array of (u_t)\n",
    "    Rewards = [] #hold an array of (r_{t+1})\n",
    "    End_state = [] #hold an array of (x_{t+1})\n",
    "    Terminal = [] #hold an array of (terminal_{t+1})\n",
    "    # Qfun( a numpy array of the obs) -> a numpy array of Q values\n",
    "    Qfun = lambda x: Q(torch.tensor(x[None,:],dtype=torch.float32))[0].numpy() \n",
    "    with torch.no_grad():\n",
    "        \n",
    "        obs = env.reset() #c)\n",
    "        for i in range(N_rollout): #c)\n",
    "            if np.random.uniform()>epsilon: #c)\n",
    "                Qnow = Qfun(obs) #c)\n",
    "                action = np.argmax(Qnow) #c)\n",
    "            else: #c)\n",
    "                action = env.action_space.sample() #c)\n",
    "            Start_state.append(obs) #c)\n",
    "            Actions.append(action) #c)\n",
    "\n",
    "            obs_next, reward, done, info = env.step(action) #c)\n",
    "            terminal = done and not info.get('TimeLimit.truncated', False) #c)\n",
    "\n",
    "            Terminal.append(terminal) #c)\n",
    "            Rewards.append(reward) #c)\n",
    "            End_state.append(obs_next) #c)\n",
    "\n",
    "            if done: #c)\n",
    "                obs = env.reset() #c)\n",
    "            else: #c)\n",
    "                obs = obs_next #c)\n",
    "                \n",
    "    #error checking:\n",
    "    assert len(Start_state)==len(Actions)==len(Rewards)==len(End_state)==len(Terminal), f'error in lengths: {len(Start_state)}=={len(Actions)}=={len(Rewards)}=={len(End_state)}=={len(Dones)}'\n",
    "    return np.array(Start_state), np.array(Actions), np.array(Rewards), np.array(End_state), np.array(Terminal).astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "81394074",
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval_Q(Q,env):\n",
    "    with torch.no_grad():\n",
    "        Qfun = lambda x: Q(torch.tensor(x[None,:],dtype=torch.float32))[0].numpy()\n",
    "        rewards_acc = 0 #d)\n",
    "        obs = env.reset() #d)\n",
    "        while True: #d)\n",
    "            action = np.argmax(Qfun(obs)) #d)\n",
    "            obs, reward, done, info = env.step(action) #d)\n",
    "            rewards_acc += reward #d)\n",
    "            if done: #d)\n",
    "                return rewards_acc #d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d759de99",
   "metadata": {},
   "outputs": [],
   "source": [
    "def DQN_rollout(Q, optimizer, env, gamma=0.98, use_target_net=False, N_iterations=21, N_rollout=20000, \\\n",
    "                N_epochs=10, batch_size=32, N_evals=10, target_net_update_feq=100):\n",
    "    best = -float('inf')\n",
    "    torch.save(Q.state_dict(),'Q-checkpoint')\n",
    "    try:\n",
    "        for iteration in range(N_iterations):\n",
    "            epsilon = 1.0 - iteration/(N_iterations-1) #e=) 1.\n",
    "            print(f'rollout iteration {iteration} with epsilon={epsilon:.2%}...')\n",
    "            \n",
    "            #2. rollout\n",
    "            Start_state, Actions, Rewards, End_state, Dones = rollout(Q, env, epsilon=epsilon, N_rollout=N_rollout) #e) 2.\n",
    "            \n",
    "            #Data conversion, no changes required\n",
    "            convert = lambda x: [torch.tensor(xi,dtype=torch.float32) for xi in x]\n",
    "            Start_state, Rewards, End_state, Dones = convert([Start_state, Rewards, End_state, Dones])\n",
    "            Actions = Actions.astype(int)\n",
    "\n",
    "            print('starting training on rollout information...')\n",
    "            t = 0\n",
    "            for epoch in range(N_epochs): \n",
    "                for i in range(batch_size,len(Start_state)+1,batch_size): \n",
    "                    if t%target_net_update_feq==0:\n",
    "                        Qtarget = deepcopy(Q) #g)\n",
    "                        pass\n",
    "                    t += 1\n",
    "                    \n",
    "                    Start_state_batch, Actions_batch, Rewards_batch, End_state_batch, Dones_batch = [d[i-batch_size:i] for d in [Start_state, Actions, Rewards, End_state, Dones]] #e=) 3.\n",
    "                    \n",
    "                    with torch.no_grad(): #3.\n",
    "                        if use_target_net:\n",
    "                            pass\n",
    "                            maxQ = torch.max(Qtarget(End_state_batch),dim=1)[0] #g)\n",
    "                        else:\n",
    "                            maxQ = torch.max(Q(End_state_batch),dim=1)[0] #e=) 3.\n",
    "                    \n",
    "#                     action_index = np.stack((np.arange(batch_size),Actions_batch),axis=0)\n",
    "                    Qnow = Q(Start_state_batch) #Q(x_t,u_t) is given\n",
    "                    \n",
    "                    Loss = torch.mean((Rewards_batch + gamma*maxQ*(1-Dones_batch) - Qnow)**2) #e) 3.\n",
    "                    optimizer.zero_grad() #e) 3.\n",
    "                    Loss.backward() #e) 3.\n",
    "                    optimizer.step() #e) 3.\n",
    "                \n",
    "                score = np.mean([eval_Q(Q,env) for i in range(N_evals)]) #e=) 3.\n",
    "                \n",
    "                print(f'iteration={iteration} epoch={epoch} Average Reward per episode:',score)\n",
    "                if score>best:\n",
    "                    best = score\n",
    "                    print('################################# \\n new best',best,'saving Q... \\n#################################')\n",
    "                    torch.save(Q.state_dict(),'Q-checkpoint')\n",
    "            \n",
    "            print('loading best result')\n",
    "            Q.load_state_dict(torch.load('Q-checkpoint'))\n",
    "    finally: #this will always run even when using the a KeyBoard Interrupt. \n",
    "        print('loading best result')\n",
    "        Q.load_state_dict(torch.load('Q-checkpoint'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "3e04fa8f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rollout iteration 0 with epsilon=100.00%...\n",
      "starting training on rollout information...\n",
      "iteration=0 epoch=0 Average Reward per episode: 6.992118896142138e-09\n",
      "################################# \n",
      " new best 6.992118896142138e-09 saving Q... \n",
      "#################################\n",
      "iteration=0 epoch=1 Average Reward per episode: 6.976118753569444e-09\n",
      "loading best result\n",
      "Unexpected exception formatting exception. Falling back to standard exception\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\20193261\\AppData\\Local\\anaconda3\\envs\\ml4sc\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3505, in run_code\n",
      "    exec(code_obj, self.user_global_ns, self.user_ns)\n",
      "  File \"C:\\Users\\20193261\\AppData\\Local\\Temp\\ipykernel_8084\\1282950012.py\", line 16, in <module>\n",
      "    DQN_rollout(Q, optimizer, env, use_target_net=True, gamma=gamma, N_iterations=N_iterations, \\\n",
      "  File \"C:\\Users\\20193261\\AppData\\Local\\Temp\\ipykernel_8084\\2654893864.py\", line 44, in DQN_rollout\n",
      "    score = np.mean([eval_Q(Q,env) for i in range(N_evals)]) #e=) 3.\n",
      "  File \"C:\\Users\\20193261\\AppData\\Local\\Temp\\ipykernel_8084\\2654893864.py\", line 44, in <listcomp>\n",
      "    score = np.mean([eval_Q(Q,env) for i in range(N_evals)]) #e=) 3.\n",
      "  File \"C:\\Users\\20193261\\AppData\\Local\\Temp\\ipykernel_8084\\2932679751.py\", line 8, in eval_Q\n",
      "    obs, reward, done, info = env.step(action) #d)\n",
      "  File \"C:\\Users\\20193261\\AppData\\Local\\anaconda3\\envs\\ml4sc\\lib\\site-packages\\gym\\wrappers\\time_limit.py\", line 18, in step\n",
      "    observation, reward, done, info = self.env.step(action)\n",
      "  File \"C:\\Users\\20193261\\AppData\\Local\\anaconda3\\envs\\ml4sc\\lib\\site-packages\\gym\\wrappers\\time_limit.py\", line 18, in step\n",
      "    observation, reward, done, info = self.env.step(action)\n",
      "  File \"C:\\Users\\20193261\\AppData\\Local\\anaconda3\\envs\\ml4sc\\lib\\site-packages\\gym_unbalanced_disk\\envs\\UnbalancedDisk.py\", line 69, in step\n",
      "  File \"C:\\Users\\20193261\\AppData\\Local\\anaconda3\\envs\\ml4sc\\lib\\site-packages\\scipy\\integrate\\_ivp\\ivp.py\", line 591, in solve_ivp\n",
      "    message = solver.step()\n",
      "  File \"C:\\Users\\20193261\\AppData\\Local\\anaconda3\\envs\\ml4sc\\lib\\site-packages\\scipy\\integrate\\_ivp\\base.py\", line 181, in step\n",
      "    success, message = self._step_impl()\n",
      "  File \"C:\\Users\\20193261\\AppData\\Local\\anaconda3\\envs\\ml4sc\\lib\\site-packages\\scipy\\integrate\\_ivp\\rk.py\", line 144, in _step_impl\n",
      "    y_new, f_new = rk_step(self.fun, t, y, self.f, h, self.A,\n",
      "  File \"C:\\Users\\20193261\\AppData\\Local\\anaconda3\\envs\\ml4sc\\lib\\site-packages\\scipy\\integrate\\_ivp\\rk.py\", line 66, in rk_step\n",
      "    y_new = y + h * np.dot(K[:-1].T, B)\n",
      "  File \"<__array_function__ internals>\", line 180, in dot\n",
      "KeyboardInterrupt\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\20193261\\AppData\\Local\\anaconda3\\envs\\ml4sc\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 2102, in showtraceback\n",
      "    stb = self.InteractiveTB.structured_traceback(\n",
      "  File \"C:\\Users\\20193261\\AppData\\Local\\anaconda3\\envs\\ml4sc\\lib\\site-packages\\IPython\\core\\ultratb.py\", line 1310, in structured_traceback\n",
      "    return FormattedTB.structured_traceback(\n",
      "  File \"C:\\Users\\20193261\\AppData\\Local\\anaconda3\\envs\\ml4sc\\lib\\site-packages\\IPython\\core\\ultratb.py\", line 1199, in structured_traceback\n",
      "    return VerboseTB.structured_traceback(\n",
      "  File \"C:\\Users\\20193261\\AppData\\Local\\anaconda3\\envs\\ml4sc\\lib\\site-packages\\IPython\\core\\ultratb.py\", line 1052, in structured_traceback\n",
      "    formatted_exception = self.format_exception_as_a_whole(etype, evalue, etb, number_of_lines_of_context,\n",
      "  File \"C:\\Users\\20193261\\AppData\\Local\\anaconda3\\envs\\ml4sc\\lib\\site-packages\\IPython\\core\\ultratb.py\", line 978, in format_exception_as_a_whole\n",
      "    frames.append(self.format_record(record))\n",
      "  File \"C:\\Users\\20193261\\AppData\\Local\\anaconda3\\envs\\ml4sc\\lib\\site-packages\\IPython\\core\\ultratb.py\", line 878, in format_record\n",
      "    frame_info.lines, Colors, self.has_colors, lvals\n",
      "  File \"C:\\Users\\20193261\\AppData\\Local\\anaconda3\\envs\\ml4sc\\lib\\site-packages\\IPython\\core\\ultratb.py\", line 712, in lines\n",
      "    return self._sd.lines\n",
      "  File \"C:\\Users\\20193261\\AppData\\Local\\anaconda3\\envs\\ml4sc\\lib\\site-packages\\stack_data\\utils.py\", line 145, in cached_property_wrapper\n",
      "    value = obj.__dict__[self.func.__name__] = self.func(obj)\n",
      "  File \"C:\\Users\\20193261\\AppData\\Local\\anaconda3\\envs\\ml4sc\\lib\\site-packages\\stack_data\\core.py\", line 698, in lines\n",
      "    pieces = self.included_pieces\n",
      "  File \"C:\\Users\\20193261\\AppData\\Local\\anaconda3\\envs\\ml4sc\\lib\\site-packages\\stack_data\\utils.py\", line 145, in cached_property_wrapper\n",
      "    value = obj.__dict__[self.func.__name__] = self.func(obj)\n",
      "  File \"C:\\Users\\20193261\\AppData\\Local\\anaconda3\\envs\\ml4sc\\lib\\site-packages\\stack_data\\core.py\", line 649, in included_pieces\n",
      "    pos = scope_pieces.index(self.executing_piece)\n",
      "  File \"C:\\Users\\20193261\\AppData\\Local\\anaconda3\\envs\\ml4sc\\lib\\site-packages\\stack_data\\utils.py\", line 145, in cached_property_wrapper\n",
      "    value = obj.__dict__[self.func.__name__] = self.func(obj)\n",
      "  File \"C:\\Users\\20193261\\AppData\\Local\\anaconda3\\envs\\ml4sc\\lib\\site-packages\\stack_data\\core.py\", line 628, in executing_piece\n",
      "    return only(\n",
      "  File \"C:\\Users\\20193261\\AppData\\Local\\anaconda3\\envs\\ml4sc\\lib\\site-packages\\executing\\executing.py\", line 164, in only\n",
      "    raise NotOneValueFound('Expected one value, found 0')\n",
      "executing.executing.NotOneValueFound: Expected one value, found 0\n"
     ]
    }
   ],
   "source": [
    "max_episode_steps = 300 # 7.5 seconds if I am not mistaken\n",
    "env = gym.make('unbalanced-disk-sincos-v0', dt=0.025, umax=3.) \n",
    "env = gym.wrappers.time_limit.TimeLimit(env,max_episode_steps=max_episode_steps)\n",
    "\n",
    "# env.reward_function(lambda env: -cos(env.theta))\n",
    "\n",
    "gamma = 0.98 #f=)\n",
    "batch_size = 32 #f=)\n",
    "N_iterations = 11 #f=)\n",
    "N_rollout = 2000 #f=)\n",
    "N_epochs = 10 #f=)\n",
    "N_evals = 5 #f=)\n",
    "lr = 0.0005 #given\n",
    "Q = Qfunction(env)\n",
    "optimizer = torch.optim.Adam(Q.parameters(),lr=lr) #low learning rate\n",
    "DQN_rollout(Q, optimizer, env, use_target_net=True, gamma=gamma, N_iterations=N_iterations, \\\n",
    "            N_rollout=N_rollout, N_epochs=N_epochs, N_evals=N_evals)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83fdab92",
   "metadata": {},
   "outputs": [],
   "source": [
    "show(Q,env)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33fc5893",
   "metadata": {},
   "outputs": [],
   "source": [
    "Rewards = [eval_Q(Q,env) for i in range(100)]\n",
    "plt.plot(Rewards,'.')\n",
    "plt.title(f'mean={np.mean(Rewards)}')\n",
    "plt.xlabel('instance')\n",
    "plt.ylabel('Reward per episode')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e88c985a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
