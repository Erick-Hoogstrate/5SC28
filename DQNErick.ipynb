{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b34da09",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import nn\n",
    "import torch, gym, gym_unbalanced_disk, time, gym.wrappers\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "from copy import deepcopy\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b693827",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Qfunction(nn.Module):\n",
    "    def __init__(self, env):\n",
    "        super(Qfunction,self).__init__()\n",
    "        self.lay1 = nn.Linear(env.observation_space.shape[0], 40)\n",
    "        self.F1 =  nn.Tanh() #a)\n",
    "        self.lay2 = nn.Linear(40,env.action_space.n)\n",
    "    \n",
    "    def forward(self, obs):\n",
    "        return self.lay2(self.F1(self.lay1(obs)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e83e83b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def show(Q,env,setting):\n",
    "    u = []\n",
    "    theta = []\n",
    "    omega = []\n",
    "    with torch.no_grad():\n",
    "        #you can use Qfun(obs) as a shorthand for the q function.\n",
    "        Qfun = lambda x: Q(torch.tensor(x[None,:],dtype=torch.float32))[0].numpy() #convert x to torch.tensor -> put in the Q function -> back to numpy\n",
    "        try:\n",
    "            obs = env.reset() #b)\n",
    "            env.render() #b)\n",
    "            time.sleep(1) #b)\n",
    "            while True: #b)\n",
    "                action = np.argmax(Qfun(obs)) #b)\n",
    "                obs, reward, done, info = env.step(action) #b)\n",
    "                time.sleep(1/60) #b)\n",
    "                env.render() #b)\n",
    "                if setting == \"u\":\n",
    "                    print(env.u)\n",
    "                elif setting == 'omega':\n",
    "                    print(env.omega)\n",
    "                elif setting == 'theta':\n",
    "                    print(env.th)\n",
    "                u.append(env.u)\n",
    "                theta.append(env.th)\n",
    "                omega.append(env.omega)\n",
    "                if done: #b)\n",
    "                    time.sleep(0.5)  #b)\n",
    "                    break  #b)\n",
    "        finally: #this will always run even when an error occurs\n",
    "            env.close()\n",
    "            return u,theta,omega"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1eeb0f15",
   "metadata": {},
   "outputs": [],
   "source": [
    "def rollout(Q, env, epsilon=0.1, N_rollout=10_000): \n",
    "    #save the following (use .append)\n",
    "    Start_state = [] #hold an array of (x_t)\n",
    "    Actions = [] #hold an array of (u_t)\n",
    "    Rewards = [] #hold an array of (r_{t+1})\n",
    "    End_state = [] #hold an array of (x_{t+1})\n",
    "    Terminal = [] #hold an array of (terminal_{t+1})\n",
    "    # Qfun( a numpy array of the obs) -> a numpy array of Q values\n",
    "    Qfun = lambda x: Q(torch.tensor(x[None,:],dtype=torch.float32))[0].numpy() \n",
    "    with torch.no_grad():\n",
    "        \n",
    "        obs = env.reset() #c)\n",
    "        for i in range(N_rollout): #c)\n",
    "            if np.random.uniform()>epsilon: #c)\n",
    "                Qnow = Qfun(obs) #c)\n",
    "                action = np.argmax(Qnow) #c)\n",
    "            else: #c)\n",
    "                action = env.action_space.sample() #c)\n",
    "            Start_state.append(obs) #c)\n",
    "            Actions.append(action) #c)\n",
    "\n",
    "            obs_next, reward, done, info = env.step(action) #c)\n",
    "            terminal = done and not info.get('TimeLimit.truncated', False) #c)\n",
    "\n",
    "            Terminal.append(terminal) #c)\n",
    "            Rewards.append(reward) #c)\n",
    "            End_state.append(obs_next) #c)\n",
    "\n",
    "            if done: #c)\n",
    "                obs = env.reset() #c)\n",
    "            else: #c)\n",
    "                obs = obs_next #c)\n",
    "                \n",
    "    #error checking:\n",
    "    assert len(Start_state)==len(Actions)==len(Rewards)==len(End_state)==len(Terminal), f'error in lengths: {len(Start_state)}=={len(Actions)}=={len(Rewards)}=={len(End_state)}=={len(Dones)}'\n",
    "    return np.array(Start_state), np.array(Actions), np.array(Rewards), np.array(End_state), np.array(Terminal).astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6431873",
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval_Q(Q,env):\n",
    "    with torch.no_grad():\n",
    "        Qfun = lambda x: Q(torch.tensor(x[None,:],dtype=torch.float32))[0].numpy()\n",
    "        rewards_acc = 0 #d)\n",
    "        obs = env.reset() #d)\n",
    "        while True: #d)\n",
    "            action = np.argmax(Qfun(obs)) #d)\n",
    "            obs, reward, done, info = env.step(action) #d)\n",
    "            rewards_acc += reward #d)\n",
    "            if done: #d)\n",
    "                return rewards_acc #d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e55a71f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def DQN_rollout(Q, optimizer, env, gamma=0.98, use_target_net=False, N_iterations=21, N_rollout=20000, \\\n",
    "                N_epochs=10, batch_size=32, N_evals=10, target_net_update_feq=100):\n",
    "    best = -float('inf')\n",
    "    torch.save(Q.state_dict(),'Q-checkpoint')\n",
    "    try:\n",
    "        for iteration in range(N_iterations):\n",
    "            epsilon = 1.0 - iteration/(N_iterations-1) #e=) 1.\n",
    "            print(f'rollout iteration {iteration} with epsilon={epsilon:.2%}...')\n",
    "            \n",
    "            #2. rollout\n",
    "            Start_state, Actions, Rewards, End_state, Dones = rollout(Q, env, epsilon=epsilon, N_rollout=N_rollout) #e) 2.\n",
    "            \n",
    "            #Data conversion, no changes required\n",
    "            convert = lambda x: [torch.tensor(xi,dtype=torch.float32) for xi in x]\n",
    "            Start_state, Rewards, End_state, Dones = convert([Start_state, Rewards, End_state, Dones])\n",
    "            Actions = Actions.astype(int)\n",
    "\n",
    "            print('starting training on rollout information...')\n",
    "            t = 0\n",
    "            for epoch in range(N_epochs): \n",
    "                for i in range(batch_size,len(Start_state)+1,batch_size): \n",
    "                    if t%target_net_update_feq==0:\n",
    "                        Qtarget = deepcopy(Q) #g)\n",
    "                        pass\n",
    "                    t += 1\n",
    "                    \n",
    "                    Start_state_batch, Actions_batch, Rewards_batch, End_state_batch, Dones_batch = [d[i-batch_size:i] for d in [Start_state, Actions, Rewards, End_state, Dones]] #e=) 3.\n",
    "                    \n",
    "                    with torch.no_grad(): #3.\n",
    "                        if use_target_net:\n",
    "                            maxQ = torch.max(Qtarget(End_state_batch),dim=1)[0] #g)\n",
    "                        else:\n",
    "                            maxQ = torch.max(Q(End_state_batch),dim=1)[0] #e=) 3.\n",
    "                    \n",
    "                    action_index = np.stack((np.arange(batch_size),Actions_batch),axis=0)\n",
    "                    Qnow = Q(Start_state_batch)[action_index] #Q(x_t,u_t) is given\n",
    "                    \n",
    "                    Loss = torch.mean((Rewards_batch + gamma*maxQ*(1-Dones_batch) - Qnow)**2) #e) 3.\n",
    "                    optimizer.zero_grad() #e) 3.\n",
    "                    Loss.backward() #e) 3.\n",
    "                    optimizer.step() #e) 3.\n",
    "                \n",
    "                score = np.mean([eval_Q(Q,env) for i in range(N_evals)]) #e=) 3.\n",
    "                \n",
    "                print(f'iteration={iteration} epoch={epoch} Average Reward per episode:',score)\n",
    "                if score>best:\n",
    "                    best = score\n",
    "                    print('################################# \\n new best',best,'saving Q... \\n#################################')\n",
    "                    torch.save(Q.state_dict(),'Q-checkpoint')\n",
    "            \n",
    "            print('loading best result')\n",
    "            Q.load_state_dict(torch.load('Q-checkpoint'))\n",
    "    finally: #this will always run even when using the a KeyBoard Interrupt. \n",
    "        print('loading best result')\n",
    "        Q.load_state_dict(torch.load('Q-checkpoint'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5be7795f",
   "metadata": {},
   "outputs": [],
   "source": [
    "max_episode_steps = 300\n",
    "env = gym.make('unbalanced-disk-sincos-v0', dt=0.025, umax=3.)\n",
    "env = gym.wrappers.time_limit.TimeLimit(env,max_episode_steps=max_episode_steps)\n",
    "\n",
    "target_angle = np.pi # target set to be balanced on top\n",
    "\n",
    "angle_reward = lambda self: ((np.cos(self.th-target_angle)+1.5)**2 - 0.25)\n",
    "correctVelocity_reward = lambda self: 0.00125*(((np.cos(self.th)+1)/2)*(self.omega)**2)\n",
    "voltage_penalty = lambda self: -0.01*((self.u)**2)\n",
    "incorrectAngle_penalty = lambda self: -0.01*np.abs(self.th-target_angle)\n",
    "\n",
    "reward_function = lambda self: angle_reward(self) + correctVelocity_reward(self) + voltage_penalty(self) + incorrectAngle_penalty(self)\n",
    "\n",
    "env.change_reward_function(reward_function)\n",
    "\n",
    "# env.set_discrete_values(discrete_size = 9, minmax = 3.0, div = 2.5, rnd = 2)\n",
    "env.set_discrete_values_manual([-3.0, -1.5, -0.75, -0.5, -0.25, -0.15, -0.1, 0, 0.1, 0.15, 0.25, 0.5, 0.75, 1.5, 3.0])\n",
    "\n",
    "gamma = 0.99 #f=)\n",
    "batch_size = 45 #f=)\n",
    "N_iterations = 18 #f=)\n",
    "N_rollout = 20000 #f=)\n",
    "N_epochs = 7 #f=)\n",
    "N_evals = 4 #f=)\n",
    "lr = 0.001 #given\n",
    "\n",
    "assert isinstance(env.action_space,gym.spaces.Discrete), 'action space requires to be discrete'\n",
    "Q = Qfunction(env)\n",
    "optimizer = torch.optim.Adam(Q.parameters(),lr=lr) #low learning rate\n",
    "DQN_rollout(Q, optimizer, env, use_target_net=True, gamma=gamma, N_iterations=N_iterations, \\\n",
    "            N_rollout=N_rollout, N_epochs=N_epochs, N_evals=N_evals)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63af2e56",
   "metadata": {},
   "outputs": [],
   "source": [
    "# max_episode_steps = 300\n",
    "# env = gym.make('unbalanced-disk-sincos-v0', dt=0.025, umax=3.)\n",
    "# env = gym.wrappers.time_limit.TimeLimit(env,max_episode_steps=max_episode_steps)\n",
    "\n",
    "# target_angle = np.pi # target set to be balanced on top\n",
    "\n",
    "\n",
    "# # reward_function =  lambda self: np.exp(-(self.th%(2*np.pi)-np.pi)**2/(2*(np.pi/7)**2))\n",
    "# # reward_function =  lambda self: (np.cos(self.th - target_angle)+1)**2  - np.cos(self.th-(np.pi+target_angle)) - 0.01*(self.omega)**2 - 0.01*(self.u)**2\n",
    "# # reward_function =  lambda self: ((np.cos(self.th-target_angle)+1.5)**2 - 0.25) + 0.00125*(((np.cos(self.th)+1)/2)*(self.omega)**2) - 0.0075*((self.u)**2) - 0.0025*((self.omega)**2)\n",
    "\n",
    "# # Washing machine (Qfunction2)\n",
    "# # reward_function = lambda self: np.exp(-(self.th % (2 * np.pi) - np.pi) ** 2 / (2 * (np.pi / 7) ** 2)) + (1.0 if np.abs(self.th % (2 * np.pi) - np.pi) < np.pi / 2 else 0.0) - 0.001 * self.u ** 2\n",
    "# # stuck at u=3\n",
    "# # reward_function = lambda self: ((np.cos(self.th-target_angle)+1.6)** 2 - 0.25) - 0.0025*(self.omega)** 2 - 0.0075*(self.u)**2\n",
    "\n",
    "# # reward_function = lambda self: np.exp(-(self.th % (2 * np.pi) - np.pi) ** 2 / (2 * (np.pi / 7) ** 2)) + (1.0 if np.abs(self.th % (2 * np.pi) - np.pi) < np.pi / 2 else -0.5) - 0.001 * self.u ** 2\n",
    "\n",
    "# # Yann best\n",
    "# reward_function =  lambda self: ((np.cos(self.th-target_angle)+1.5)**2 - 0.25) + 0.00125*(((np.cos(self.th)+1)/2)*(self.omega)**2) - 0.01*((self.u)**2)\n",
    "\n",
    "# env.change_reward_function(reward_function)\n",
    "\n",
    "# env.set_discrete_values(discrete_size = 9, minmax = 3.0, div = 2.5, rnd = 2)\n",
    "# # env.set_discrete_values_manual([-3.0, -1.5, -0.75, -0.5, -0.2, -0.1, -0.05, 0, 0.05, 0.1, 0.2, 0.5, 0.75, 1.5, 3.0])\n",
    "\n",
    "# # gamma = 0.98 #f=)\n",
    "# # batch_size = 32 #f=)\n",
    "# # N_iterations = 21 #f=)\n",
    "# # N_rollout = 20000 #f=)\n",
    "# # N_epochs = 10 #f=)\n",
    "# # N_evals = 5 #f=)\n",
    "# # lr = 0.0005 #given\n",
    "# gamma = 0.98 #f=)\n",
    "# batch_size = 64 #f=)\n",
    "# N_iterations = 41 #f=)\n",
    "# N_rollout = 50000 #f=)\n",
    "# N_epochs = 50 #f=)\n",
    "# N_evals = 10 #f=)\n",
    "# lr = 0.0005 #given\n",
    "\n",
    "# assert isinstance(env.action_space,gym.spaces.Discrete), 'action space requires to be discrete'\n",
    "# Q = Qfunction(env)\n",
    "# optimizer = torch.optim.Adam(Q.parameters(),lr=lr) #low learning rate\n",
    "# DQN_rollout(Q, optimizer, env, use_target_net=False, gamma=gamma, N_iterations=N_iterations, \\\n",
    "#             N_rollout=N_rollout, N_epochs=N_epochs, N_evals=N_evals)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "180bf35c",
   "metadata": {},
   "outputs": [],
   "source": [
    "u,theta,omega = show(Q,env,'theta')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f026d854",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a figure with three subplots\n",
    "fig, axs = plt.subplots(1, 3, figsize=(12, 4))\n",
    "\n",
    "# Plot theta\n",
    "axs[0].plot(theta, '.')\n",
    "axs[0].set_title('Theta')\n",
    "axs[0].set_xlabel('instance')\n",
    "axs[0].set_ylabel('Angle')\n",
    "\n",
    "# Plot u\n",
    "axs[1].plot(u, '.')\n",
    "axs[1].set_title('u')\n",
    "axs[1].set_xlabel('instance')\n",
    "axs[1].set_ylabel('Input voltage')\n",
    "\n",
    "# Plot omega\n",
    "axs[2].plot(omega, '.')\n",
    "axs[2].set_title('omega')\n",
    "axs[2].set_xlabel('instance')\n",
    "axs[2].set_ylabel('Speed')\n",
    "\n",
    "# Adjust spacing between subplots\n",
    "plt.tight_layout()\n",
    "\n",
    "# Show the plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8139785d",
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_Q(Q,env)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc7a5889",
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "Rewards = [eval_Q(Q,env) for i in range(100)]\n",
    "plt.plot(Rewards,'.')\n",
    "plt.title(f'mean={np.mean(Rewards)}')\n",
    "plt.xlabel('instance')\n",
    "plt.ylabel('Reward per episode')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d3f0815",
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_theta(env, theta, basis_fun):\n",
    "    # for a given enviroment, theta matrix (Nbasis, Naction) and basis_fun(obs) -> (Nbasis,) \n",
    "    # it visualizes the max Q value in state-space.\n",
    "    low, high = env.observation_space.low, env.observation_space.high\n",
    "    nvec = [50,60]\n",
    "    Xvec = [np.linspace(l,h,num=ni) for l,h,ni in zip(low,high,nvec)] # calculate the linspace in all directions\n",
    "    c_points = np.array(np.meshgrid(*Xvec)) # meshgrid all the linspaces together (Nx, X1, X2, X3, ...) \n",
    "    c_points = np.moveaxis(c_points, 0, -1) #transform to (X1, X2, X3, ..., Nobs) \n",
    "    c_points = c_points.reshape((-1,c_points.shape[-1])) #flatten into the size (Nc, Nobs)\n",
    "    maxtheta = np.array([np.max(basis_fun(ci)@theta) for ci in c_points]).reshape((nvec[1],nvec[0]))\n",
    "    \n",
    "    plt.contour(Xvec[0],Xvec[1],maxtheta)\n",
    "    plt.xlabel('position')\n",
    "    plt.ylabel('velocity')\n",
    "    plt.colorbar()\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "def make_radial_basis_network(env,nvec,scale):\n",
    "    # env: is the given enviroment\n",
    "    # nvec: is the given number of grid points in each dimention.\n",
    "    # scale: is the sigma_c in the equation\n",
    "    if isinstance(nvec,int):\n",
    "        nvec = [nvec]*env.observation_space.shape[0]\n",
    "    \n",
    "    # This creates a grid of points c_i the lower bound to the upper bound with nvec number of samples in each dimention\n",
    "    low, high = env.observation_space.low, env.observation_space.high # get upper and lower bound\n",
    "    assert np.all(np.isfinite(low)) and np.all(np.isfinite(high)), f'infinite bounds on obersvation space are not permitted low={low}, high={high}'\n",
    "    Xvec = [np.linspace(l,h,num=ni) for l,h,ni in zip(low,high,nvec)] # calculate the linspace in all directions\n",
    "    c_points = np.array(np.meshgrid(*Xvec)) # meshgrid all the linspaces together (Nx, X1, X2, X3, ...) \n",
    "    c_points = np.moveaxis(c_points, 0, -1) #transform to (X1, X2, X3, ..., Nobs) \n",
    "    c_points = c_points.reshape((-1,c_points.shape[-1])) #flatten into the size (Nc, Nobs)\n",
    "    dx = np.array([X[1]-X[0] for X in Xvec]) # spacing (related to the B matrix)\n",
    "    \n",
    "    def basis_fun(obs):\n",
    "        #this function should return the vector containing all phi_i of all c_points\n",
    "        obs = np.array(obs) #(Nobs)\n",
    "        \n",
    "        dis = (c_points-obs[None,:])/dx[None,:] #dim = (Nbasis, Nobs)\n",
    "        exp_arg = np.sum(dis**2,axis=1)/(2*scale**2) #squared distance to every point #b)\n",
    "        Z = -exp_arg+np.min(exp_arg) #b) for numerical stability you can add the minimum.\n",
    "        R = np.exp(Z) #b)\n",
    "        return R/np.sum(R) #b)\n",
    "    \n",
    "    return basis_fun #returns a basis function\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e18f26c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# nvec = 10\n",
    "# scale = 0.5\n",
    "# basis_fun = make_radial_basis_network(env,nvec,scale=scale) #e)\n",
    "# visualize_theta(env, theta, basis_fun) #d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53918060",
   "metadata": {},
   "outputs": [],
   "source": [
    "Q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21de9071",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save Q to a file\n",
    "torch.save(Q.state_dict(), 'QfunctionYann5.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0b8e503",
   "metadata": {},
   "outputs": [],
   "source": [
    "env.action_space.n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48e71adc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# max_episode_steps = 200\n",
    "# env = gym.make('unbalanced-disk-sincos-v0', dt=0.025, umax=3.)\n",
    "# env = gym.wrappers.time_limit.TimeLimit(env,max_episode_steps=max_episode_steps)\n",
    "# env.set_discrete_values(discrete_size = 13, minmax = 3.0, div = 2.5, rnd = 2)\n",
    "\n",
    "# target_angle = np.pi # target set to be balanced on top\n",
    "\n",
    "# gamma = 0.98 #f=)\n",
    "# batch_size = 32 #f=)\n",
    "# N_iterations = 21 #f=)\n",
    "# N_rollout = 20000 #f=)\n",
    "# N_epochs = 10 #f=)\n",
    "# N_evals = 5 #f=)\n",
    "# lr = 0.0005 #given\n",
    "\n",
    "# a_list = [0.5,1,5]\n",
    "# b_list = [0.1, 0.01, 0.001]\n",
    "# c_list = [0.1, 0.01, 0.001]\n",
    "\n",
    "# results_table = np.zeros((len(a_list),len(b_list),len(c_list)))\n",
    "\n",
    "# best_result = -float('inf') \n",
    "# best_result_idx = [0,0,0]\n",
    "\n",
    "# for i, a in enumerate(a_list):\n",
    "#     for j, b in enumerate(b_list):\n",
    "#         for k, c in enumerate(c_list):\n",
    "#             print(f'Currently running a = {a}, b = {b} and c = {c}')\n",
    "#             env_it = deepcopy(env)\n",
    "#             reward_function =  lambda self: -(a*(np.abs(self.th)-np.abs(target_angle))**2 + b*(self.omega)**2 + c*(self.u)**2)\n",
    "#             env_it.change_reward_function(reward_function)\n",
    "            \n",
    "#             assert isinstance(env.action_space,gym.spaces.Discrete), 'action space requires to be discrete'\n",
    "#             Q = Qfunction(env_it)\n",
    "#             optimizer = torch.optim.Adam(Q.parameters(),lr=lr) #low learning rate\n",
    "#             DQN_rollout(Q, optimizer, env_it, use_target_net=True, gamma=gamma, N_iterations=N_iterations, \\\n",
    "#                         N_rollout=N_rollout, N_epochs=N_epochs, N_evals=N_evals)\n",
    "            \n",
    "#             result = eval_Q(Q,env_it)\n",
    "#             results_table[i,j,k] = result\n",
    "            \n",
    "#             print(f'Result = {result}')\n",
    "            \n",
    "#             if result > best_result:\n",
    "#                 best_result = result\n",
    "#                 best_result_idx = [i,j,k]\n",
    "#                 print(f'################################# \\n new best',result,'saving result... \\n#################################')\n",
    "\n",
    "\n",
    "# print(best_result)\n",
    "# print(results_table)\n",
    "            "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
